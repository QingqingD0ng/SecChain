{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1fccf06-418f-48c2-85f6-9f8e6e934af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from pgpt_python.client import PrivateGPTApi\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "from llama_index.core.evaluation import RelevancyEvaluator, CorrectnessEvaluator, FaithfulnessEvaluator\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9af6f6-aa9b-4d19-ae68-ca7a6e6b4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = PrivateGPTApi(base_url='http://localhost:8001')\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28488532-2409-45ee-a76d-a2869668ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/', exist_ok=True)\n",
    "# Load data\n",
    "reader = SimpleDirectoryReader('./data/')\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba432076-ae22-402a-87bc-cb2613ecda9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qingqingdong/.pyenv/versions/private_env/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:215: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.30566223168627227 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "/Users/qingqingdong/.pyenv/versions/private_env/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:312: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "# Create vector index\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e367cb8-7a47-4ae3-961c-94a3864602cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the dictionary from the local JSON file\n",
    "with open('questions_answers.json', 'r') as f:\n",
    "    questions_answers = json.load(f)\n",
    "\n",
    "# Convert the dictionary to a list of questions and answers using a list comprehension\n",
    "questions_answers_list = [(q, questions_answers[q]) for q in questions_answers]\n",
    "\n",
    "# Extract the questions and answers from the list\n",
    "eval_questions, answers = zip(*questions_answers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b31cc05-1bd4-4b20-a33c-a207178344e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reference answers for evaluation using PrivateGPT\n",
    "system_prompt_context = '''\n",
    "    Instruction: You are an IT security expert for a famous German company called Hydac.\n",
    "    Given the question and related context, generate a short, clear, and professional response in German.\n",
    "    You can refer to the example give below -----\n",
    "    Question: Inwieweit werden alle Mitarbeiter zur Einhaltung der Informationssicherheit verpflichtet?\n",
    "    Context: 2.1.2 Inwieweit werden alle Mitarbeiter zur Einhaltung der Informationssicherheit verpflichtet?\n",
    "    Detaillierte Sachverhaltsdarstellung (inkl. Beurteilungsverfahren) Betrachtete Dokumente/Nachweise/Prüfungshandlung:\n",
    "    Datengeheimnis Arbeitsvertrag,  IT-SRL, Vorlagen  Datenschutz Statement Hydac: Im Arbeitsvertrag wird auf die Einhaltung\n",
    "    von Verfahrensanweisungen und im Unternehmen geltenden Richtlinien sowie Geheimhaltung verpflichtet. Mit Abschluss des Arbeitsvertrages \n",
    "    erhält der Mitarbeiter eine Erklärung zum Datengeheimnis, die er mit dem Unterzeichnen einwilligt. Weitere Verpflichtungen zur Geheimhaltung \n",
    "    können je nach Berufsgruppe bestehen und werden durch den Zentralbereich Datenschutz ausgegeben. Die IT-SRL ist eine Verfahrensanweisung \n",
    "    und somit für alle Mitarbeiter bindend. Mitarbeiter werden zur Geheimhaltung und auf das Regelwerk zur Informationssicherheit verpflichtet. \n",
    "    Feststellung Auf Basis der Beobachtungen wurde keine Abweichung festgestellt. \n",
    "    Answer: Im Arbeitsvertrag wird auf die Einhaltung von Verfahrensanweisungen und im Unternehmen geltenden Richtlinien sowie Geheimhaltung \n",
    "    verpflichtet. Mit Abschluss des Arbeitsvertrages erhält der Mitarbeiter eine Erklärung zum Datengeheimnis, die er mit dem Unterzeichnen einwilligt. \n",
    "    Weitere Verpflichtungen zur Geheimhaltung können je nach Berufsgruppe bestehen und werden durch den Zentralbereich Datenschutz ausgegeben. \n",
    "    Die IT-SRL ist eine Verfahrensanweisung und somit für alle Mitarbeiter bindend Mitarbeiter werden zur Geheimhaltung und auf das Regelwerk zur Informationssicherheit verpflichtet..\n",
    "'''\n",
    "\n",
    "system_prompt_without_context = '''\n",
    "    Instruction: You are an IT security expert for a famous German company called Hydac.\n",
    "    Given the question, generate a short, clear, and professional response in German.\n",
    "    You can refer to the example give below -----\n",
    "    Question: Inwieweit werden alle Mitarbeiter zur Einhaltung der Informationssicherheit verpflichtet?\n",
    "    Answer: Im Arbeitsvertrag wird auf die Einhaltung von Verfahrensanweisungen und im Unternehmen geltenden Richtlinien sowie Geheimhaltung \n",
    "    verpflichtet. Mit Abschluss des Arbeitsvertrages erhält der Mitarbeiter eine Erklärung zum Datengeheimnis, die er mit dem Unterzeichnen einwilligt. \n",
    "    Weitere Verpflichtungen zur Geheimhaltung können je nach Berufsgruppe bestehen und werden durch den Zentralbereich Datenschutz ausgegeben. \n",
    "    Die IT-SRL ist eine Verfahrensanweisung und somit für alle Mitarbeiter bindend Mitarbeiter werden zur Geheimhaltung und auf das Regelwerk zur Informationssicherheit verpflichtet..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66941214-01a9-4ca4-bc04-b95319c0c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_answers = []\n",
    "for question in eval_questions:\n",
    "    response = client.contextual_completions.prompt_completion(\n",
    "        prompt=question,\n",
    "        use_context=True,\n",
    "        include_sources=True,\n",
    "        system_prompt=system_prompt_context\n",
    "    ).choices[0]\n",
    "    reference_answers.append({\n",
    "        \"content\": response.message.content,\n",
    "        \"sources\": [c.text for c in response.sources]\n",
    "    })\n",
    "    # Write the updated list to the file\n",
    "    with open('answers.json', 'w') as f:\n",
    "        json.dump(reference_answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6c688-a2af-44bd-8bf0-8d5b45aee42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_answers = []\n",
    "for question in eval_questions:\n",
    "    response = client.contextual_completions.prompt_completion(\n",
    "        prompt=question,\n",
    "        system_prompt=system_prompt_without_context\n",
    "    ).choices[0]\n",
    "    reference_answers.append({\n",
    "        \"content\": response.message.content,\n",
    "    })\n",
    "    # Write the updated list to the file\n",
    "    with open('llama3_answers_zeroshot.json', 'w') as f:\n",
    "        json.dump(reference_answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c25f576-401f-46be-87f4-e9ee1a7426d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judges\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator,\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "judges = {}\n",
    "\n",
    "judges[\"answer_relevancy\"] = AnswerRelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    ")\n",
    "\n",
    "# judges[\"context_relevancy\"] = ContextRelevancyEvaluator(\n",
    "#     llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "# )\n",
    "\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    ")\n",
    "\n",
    "# judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "#     llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d5ac41f-1114-4836-981d-6bbc40a71d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama3_answers_zeroshot.json', 'r') as f:\n",
    "    reference_answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2aa916f8-f6d7-4875-98f7-a132ccc037b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tasks = []\n",
    "for i in range(len(eval_questions)):\n",
    "    eval_tasks.append(\n",
    "        judges[\"answer_relevancy\"].aevaluate(\n",
    "            query=eval_questions[i],\n",
    "            response=reference_answers[i]['content'],\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n",
    "    eval_tasks.append(\n",
    "        judges[\"correctness\"].aevaluate(\n",
    "            query=eval_questions[i],\n",
    "            response=reference_answers[i]['content'],\n",
    "            reference=answers[i],\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n",
    "    # eval_tasks.append(\n",
    "    #     judges[\"context_relevancy\"].aevaluate(\n",
    "    #         query=eval_questions[i],\n",
    "    #         contexts=reference_answers[i]['sources'],\n",
    "    #         sleep_time_in_seconds=1.0,\n",
    "    #     )\n",
    "    # )\n",
    "    # eval_tasks.append(\n",
    "    #     judges[\"faithfulness\"].aevaluate(\n",
    "    #         response=reference_answers[i]['content'],\n",
    "    #         contexts=reference_answers[i]['sources'],\n",
    "    #         sleep_time_in_seconds=1.0,\n",
    "    # )\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4deed332-b8ca-45e8-ad9c-f9896c85b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 82/82 [00:09<00:00,  8.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.asyncio import tqdm_asyncio\n",
    "llama3_zeroshot_eval_results = await tqdm_asyncio.gather(*eval_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a776da8-9c56-478a-bd06-3d74d2940e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi3_evals = {\n",
    "#     \"answer_relevancy\": eval_results[::4],\n",
    "#      \"correctness\":eval_results[1::4],\n",
    "#     \"context_relevancy\": eval_results[2::4],\n",
    "#      \"faithfulness\": eval_results[3::4],\n",
    "# }\n",
    "# mistral_evals = {\n",
    "#     \"answer_relevancy\": mistral_eval_results[::4],\n",
    "#      \"correctness\":mistral_eval_results[1::4],\n",
    "#     \"context_relevancy\": mistral_eval_results[2::4],\n",
    "#      \"faithfulness\": mistral_eval_results[3::4],\n",
    "# }\n",
    "# llama3_evals = {\n",
    "#     \"answer_relevancy\": llama3_eval_results[::4],\n",
    "#      \"correctness\":llama3_eval_results[1::4],\n",
    "#     \"context_relevancy\": llama3_eval_results[2::4],\n",
    "#      \"faithfulness\": llama3_eval_results[3::4],\n",
    "# }\n",
    "# gemma_evals = {\n",
    "#     \"answer_relevancy\": gemma_eval_results[::4],\n",
    "#      \"correctness\":gemma_eval_results[1::4],\n",
    "#     \"context_relevancy\": gemma_eval_results[2::4],\n",
    "#      \"faithfulness\": gemma_eval_results[3::4],\n",
    "# }\n",
    "llama3_zero_shot_evals = {\n",
    "    \"answer_relevancy\": llama3_zeroshot_eval_results[::2],\n",
    "     \"correctness\":llama3_zeroshot_eval_results[1::2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c97904c4-8950-4009-a1ac-451cc015be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "import pandas as pd\n",
    "\n",
    "deep_dfs = {}\n",
    "mean_dfs = {}\n",
    "for metric in phi3_evals.keys():\n",
    "    deep_df, mean_df = get_eval_results_df(\n",
    "        names=[\"baseline\"] * len(phi3_evals[metric]),\n",
    "        results_arr=phi3_evals[metric],\n",
    "        metric=metric,\n",
    "    )\n",
    "    deep_dfs[metric] = deep_df\n",
    "    mean_dfs[metric] = mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23ce33f1-fa8d-4a10-8eff-e37b66703225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.798780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                           baseline\n",
       "metrics                               \n",
       "mean_answer_relevancy_score   0.963415\n",
       "mean_correctness_score        4.097561\n",
       "mean_context_relevancy_score  0.798780\n",
       "mean_faithfulness_score       1.000000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phi3\n",
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0b35b-e26a-475a-89ec-91319f8724c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c186879-d00d-4728-9b94-bc702cdf3068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bb89b-2e17-4f09-a09e-438d90847a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e626987-4b9c-4e36-9b7b-a4e98105c84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.182927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.832317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                           baseline\n",
       "metrics                               \n",
       "mean_answer_relevancy_score   0.963415\n",
       "mean_correctness_score        4.182927\n",
       "mean_context_relevancy_score  0.832317\n",
       "mean_faithfulness_score       1.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mistral\n",
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6338102d-ad03-48e5-87cf-1b63b06d1cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.182927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.844512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                           baseline\n",
       "metrics                               \n",
       "mean_answer_relevancy_score   0.963415\n",
       "mean_correctness_score        4.182927\n",
       "mean_context_relevancy_score  0.844512\n",
       "mean_faithfulness_score       1.000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama3\n",
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "132a27e1-9263-4ad9-b353-29c41499cff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.085366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.814024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                           baseline\n",
       "metrics                               \n",
       "mean_answer_relevancy_score   0.951220\n",
       "mean_correctness_score        4.085366\n",
       "mean_context_relevancy_score  0.814024\n",
       "mean_faithfulness_score       1.000000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gemma\n",
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "004b72f1-95d9-40da-8a38-61494bcee5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>3.914634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                          baseline\n",
       "metrics                              \n",
       "mean_answer_relevancy_score  0.951220\n",
       "mean_correctness_score       3.914634"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama3_zero_shot\n",
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d0bd2-3336-400c-ab81-4519d40e8785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d2e936e6-9c3f-4c9b-a016-ce11f8f6e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "js = {}\n",
    "for i in llama3_zero_shot_evals:\n",
    "        js[i] = [j.json() for j in llama3_zero_shot_evals[i]]\n",
    "with open('llama3_zero_shot_evals.json', 'w') as f:\n",
    "        json.dump(js, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b9cc00f9-7fa4-40e2-8417-59834b8f55f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama3</th>\n",
       "      <td>4.182927</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral</th>\n",
       "      <td>4.182927</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.832317</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi3</th>\n",
       "      <td>4.097561</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma</th>\n",
       "      <td>4.085366</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3_zero_shot</th>\n",
       "      <td>3.914634</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  correctness  answer_relevancy  context_relevancy  \\\n",
       "model                                                                \n",
       "llama3               4.182927          0.963415           0.844512   \n",
       "mistral              4.182927          0.963415           0.832317   \n",
       "phi3                 4.097561          0.963415           0.798780   \n",
       "gemma                4.085366          0.951220           0.814024   \n",
       "llama3_zero_shot     3.914634          0.951220                NaN   \n",
       "\n",
       "                  faithfulness  \n",
       "model                           \n",
       "llama3                     1.0  \n",
       "mistral                    1.0  \n",
       "phi3                       1.0  \n",
       "gemma                      1.0  \n",
       "llama3_zero_shot           NaN  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_evals(evals,metrics = [\"correctness\",'answer_relevancy', 'context_relevancy',\"faithfulness\"]):\n",
    "    deep_dfs = {}\n",
    "    mean_dfs = {}\n",
    "    for metric in metrics:\n",
    "        deep_df, mean_df = get_eval_results_df(\n",
    "            names=[\"baseline\"] * len(evals[metric]),\n",
    "            results_arr=evals[metric],\n",
    "            metric=metric,\n",
    "        )\n",
    "        deep_dfs[metric] = deep_df\n",
    "        mean_dfs[metric] = mean_df\n",
    "    mean_scores_df = pd.concat(\n",
    "        [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "    return mean_scores_df\n",
    "\n",
    "# Process each model's evaluation results\n",
    "\n",
    "\n",
    "llama3_df = process_evals(llama3_evals)\n",
    "mistral_df = process_evals(mistral_evals)\n",
    "phi3_df = process_evals(phi3_evals)\n",
    "gemma_df = process_evals(gemma_evals)\n",
    "llama3_zero_shot_df = process_evals(llama3_zero_shot_evals,metrics = [\"correctness\",'answer_relevancy'])\n",
    "\n",
    "# Add a column for model names\n",
    "\n",
    "llama3_df['model'] = 'llama3'\n",
    "mistral_df['model'] = 'mistral'\n",
    "phi3_df['model'] = 'phi3'\n",
    "gemma_df['model'] = 'gemma'\n",
    "llama3_zero_shot_df['model'] = 'llama3_zero_shot'\n",
    "\n",
    "# Combine all dataframes into a single dataframe\n",
    "combined_df = pd.concat([ llama3_df, mistral_df, phi3_df,gemma_df, llama3_zero_shot_df])\n",
    "\n",
    "# Pivot the dataframe to have models as rows and metrics as columns\n",
    "pivot_df = combined_df.pivot_table(index='model', columns='metrics', values='baseline')\n",
    "\n",
    "pivot_df.columns = [\"answer_relevancy\",\"context_relevancy\",'correctness','faithfulness']\n",
    "\n",
    "pivot_df = pivot_df[[\"correctness\",'answer_relevancy', 'context_relevancy',\"faithfulness\"]]\n",
    "\n",
    "pivot_df = pivot_df.sort_values(by='correctness',ascending=False)\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "878c24df-9035-4f1b-a422-f8c0b9d10cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pivot_df.columns = ['answer_relevancy', 'context_relevancy',\"correctness\",\"faithfulness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2824490b-87e4-42fe-8243-684e3929cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.to_csv('evaluation.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5339d-3eb8-447e-988c-63de18b7a8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "private_env",
   "language": "python",
   "name": "private_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
